{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get(generate_link())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://m.media-amazon.com/images/I/517I-u-1NGL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/I/51POjQXrnVL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/I/51ywcR6OqkL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/I/51-D+0blRnS._SL500_.jpg\n",
      "https://m.media-amazon.com/images/I/51Xt2BYA5vL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/I/61CyC23FFwL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/I/51g3AinAJNL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/I/51pCOQAUu4L._SL500_.jpg\n",
      "https://m.media-amazon.com/images/I/51NyNt9PePS._SL500_.jpg\n",
      "https://m.media-amazon.com/images/I/51qO2LV-ilL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/I/515rqFN7PJL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/I/61i0TYaZ9pL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/I/614otPUQ5bL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/I/51HAoKblnpL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/I/51TjnQD6ILL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/I/51FvcRRvyUL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/I/61tqfwb3YML._SL500_.jpg\n",
      "https://m.media-amazon.com/images/I/61h58kY9lsL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/I/51B+tOzmoAL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/I/61ltix+WxRL._SL500_.jpg\n",
      "Success: 20 images found\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the URL of this page\n",
    "def generate_link(page=1, audible_programs=\"20956260011\", author_author=\"\", keywords=\"\", narrator=\"full-cast\", publisher=\"\", sort=\"review-rank\", title=\"\", pageSize=100):\n",
    "  base_url = \"https://www.audible.com/search?\"\n",
    "  params = {\n",
    "    \"audible_programs\": audible_programs,\n",
    "    \"author_author\": author_author,\n",
    "    \"keywords\": keywords,\n",
    "    \"narrator\": narrator,\n",
    "    \"pageSize\": pageSize,\n",
    "    \"publisher\": publisher,\n",
    "    \"sort\": sort,\n",
    "    \"title\": title,\n",
    "    \"ref\": \"a_search_l1_audible_programs_0\",\n",
    "    \"pf_rd_p\": \"daf0f1c8-2865-4989-87fb-15115ba5a6d2\",\n",
    "    \"pf_rd_r\": \"3CSM3Q3AG46QRQ0TVK0F\",\n",
    "    \"pageLoadId\": \"dELu6hUurPGV8fAu\",\n",
    "    \"creativeId\": \"9648f6bf-4f29-4fb4-9489-33163c0bb63e\"\n",
    "  }\n",
    "  if page > 1:\n",
    "    params[\"page\"] = page\n",
    "  query = \"&\".join([f\"{key}={value}\" for key, value in params.items()])\n",
    "  return base_url + query\n",
    "\n",
    "\n",
    "def scrape_all_details(page):\n",
    "  # Send a GET request to the page and parse the HTML content\n",
    "  response = requests.get(page)\n",
    "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "  # Find all the elements that contain the product details\n",
    "  products = soup.find_all(\"div\", class_=\"bc-col-responsive bc-col-6\")\n",
    "\n",
    "  # Create an empty list to store the details\n",
    "  details_list = []\n",
    "\n",
    "  img_tags = soup.find_all(\"img\")\n",
    "  # list of image\n",
    "  urls = []\n",
    "  # Loop through the img tags and get the src attribute of each one\n",
    "  for i, img_tag in enumerate(img_tags):\n",
    "    try:\n",
    "      src = img_tag[\"src\"]\n",
    "      # print(src) # Print the image URL\n",
    "      urls.append(src)\n",
    "\n",
    "    except:\n",
    "      src = \"N/A\"\n",
    "      urls.append(src)\n",
    "      # print(src) # Print the image URL\n",
    "  cover_image = []\n",
    "  for image_link in urls:\n",
    "    if \"https://m.media-amazon.com/images/I\" in image_link or \".jpg\" in image_link:\n",
    "      print(image_link)\n",
    "      cover_image.append(image_link)\n",
    "  if len(cover_image) % 10 != 0:\n",
    "    print(f\"Error: {len(cover_image)} images found\")\n",
    "    return None\n",
    "  else:\n",
    "    print(f\"Success: {len(cover_image)} images found\")\n",
    "\n",
    "# Loop through each product element and extract the details\n",
    "  for product in products:\n",
    "    # Try to find the title element and handle the exception if not found\n",
    "    try:\n",
    "      title = product.find(\"h3\", class_=\"bc-heading\").text.strip()\n",
    "    except AttributeError:\n",
    "      title = \"N/A\"\n",
    "      continue\n",
    "    # Try to find the subtitle element and handle the exception if not found\n",
    "    try:\n",
    "      subtitle = product.find(\"span\", class_=\"subtitle\").text.strip()\n",
    "    except AttributeError:\n",
    "      subtitle = \"N/A\"\n",
    "    # Try to find the author element and handle the exception if not found\n",
    "    try:\n",
    "      author = product.find(\"li\", class_=\"authorLabel\").text.strip()\n",
    "    except AttributeError:\n",
    "      author = \"N/A\"\n",
    "    # Try to find the narrator element and handle the exception if not found\n",
    "    try:\n",
    "      narrator = product.find(\"li\", class_=\"narratorLabel\").text.strip()\n",
    "    except AttributeError:\n",
    "      narrator = \"N/A\"\n",
    "    try:\n",
    "      series = product.find(\"li\", class_=\"seriesLabel\").text.strip()\n",
    "    except AttributeError:\n",
    "      series = \"N/A\"\n",
    "    try:\n",
    "      length = product.find(\"li\", class_=\"runtimeLabel\").text.strip()\n",
    "    except AttributeError:\n",
    "      length = \"N/A\"\n",
    "    try:\n",
    "      release_date = product.find(\"li\", class_=\"releaseDateLabel\").text.strip() \n",
    "    except AttributeError:\n",
    "      release_date = \"N/A\"\n",
    "    try:\n",
    "      language = product.find(\"li\", class_=\"languageLabel\").text.strip()\n",
    "    except AttributeError:\n",
    "      language = \"N/A\"\n",
    "\n",
    "    try:\n",
    "      ratings = product.find(\"li\", class_=\"ratingsLabel\").text.strip()\n",
    "    except AttributeError:\n",
    "      ratings = \"N/A\"\n",
    "\n",
    "    # Try to find the summary element and handle the exception if not found\n",
    "    try:\n",
    "      summary = product.find(\"p\", class_=\"bc-text\").text.strip()\n",
    "    except AttributeError:\n",
    "      summary = \"N/A\"\n",
    "\n",
    "    image = \"N/A\"\n",
    "\n",
    "    # Try to find the link element and handle the exception if not found\n",
    "    try:\n",
    "      link = product.find(\"a\", class_=\"bc-link bc-color-link\").get(\"href\")\n",
    "    except AttributeError:\n",
    "      link = \"N/A\"\n",
    "\n",
    "    # Create a dictionary with the product details\n",
    "    details_dict = {\n",
    "      \"title\": title,\n",
    "      \"subtitle\": subtitle,\n",
    "      \"author\": author,\n",
    "      \"narrator\": narrator,\n",
    "      \"series\": series,\n",
    "      \"length\": length,\n",
    "      \"release_date\": release_date,\n",
    "      \"language\": language,\n",
    "      \"ratings\": ratings,\n",
    "      \"summary\": summary,\n",
    "      \"image\": image, # Add this line\n",
    "      \"link\": link # Add this line\n",
    "    }\n",
    "\n",
    "    # Format the values using strip and replace methods\n",
    "    for key, value in details_dict.items():\n",
    "      # Remove leading and trailing whitespaces\n",
    "      value = value.strip()\n",
    "      # Replace multiple whitespaces with a single space using re.sub\n",
    "      value = re.sub(\"\\s+\", \" \", value)\n",
    "      # Update the dictionary with the formatted value\n",
    "      details_dict[key] = value\n",
    "\n",
    "    # Append the dictionary to the list\n",
    "    details_list.append(details_dict)\n",
    "  \n",
    "  # add cover image to the dictionary in the list\n",
    "  for i in range(len(details_list)):\n",
    "    details_list[i][\"image\"] = cover_image[i]\n",
    "\n",
    "  # Return the list with all the details\n",
    "  return details_list\n",
    "\n",
    "data = scrape_all_details(generate_link())\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sqlite3 module\n",
    "import sqlite3\n",
    "\n",
    "# Create a connection to a SQLite database file\n",
    "conn = sqlite3.connect(\"audiobooks.db\")\n",
    "\n",
    "# Create a cursor object to execute SQL commands\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create a table to store the data using the CREATE TABLE statement\n",
    "sql = \"\"\"\n",
    "cur.execute(sql)\n",
    "CREATE TABLE audiobooks (\n",
    "  id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "  title TEXT,\n",
    "  subtitle TEXT,\n",
    "  author TEXT,\n",
    "  narrator TEXT,\n",
    "  series TEXT,\n",
    "  length INTEGER,\n",
    "  release_date TEXT,\n",
    "  language TEXT,\n",
    "  ratings REAL,\n",
    "  summary TEXT,\n",
    "  image TEXT,\n",
    "  link TEXT,\n",
    ");\n",
    ")\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# -- then you can use INSERT OR IGNORE to skip existing data\n",
    "for item in data:\n",
    "  cur.execute(\"\"\"INSERT OR IGNORE INTO audiobooks (\n",
    "    title,\n",
    "    subtitle,\n",
    "    author,\n",
    "    narrator,\n",
    "    series,\n",
    "    length,\n",
    "    release_date,\n",
    "    language,\n",
    "    ratings,\n",
    "    summary,\n",
    "    image,\n",
    "    link\n",
    "  ) VALUES (?,?,?,?,?,?,?,?,?,?,?,?)\"\"\", (\n",
    "    item[\"title\"],\n",
    "    item[\"subtitle\"],\n",
    "    item[\"author\"],\n",
    "    item[\"narrator\"],\n",
    "    item[\"series\"],\n",
    "    item[\"length\"],\n",
    "    item[\"release_date\"],\n",
    "    item[\"language\"],\n",
    "    item[\"ratings\"],\n",
    "    item[\"summary\"],\n",
    "    item[\"image\"],\n",
    "    item[\"link\"]\n",
    "  ))\n",
    "\n",
    "\n",
    "# Commit the changes to the database\n",
    "conn.commit()\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "no such table: audiobooks",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\saket\\Documents\\GitHub\\Pyhton\\web scraping\\test.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/saket/Documents/GitHub/Pyhton/web%20scraping/test.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m conn \u001b[39m=\u001b[39m sqlite3\u001b[39m.\u001b[39mconnect(\u001b[39m\"\u001b[39m\u001b[39maudiobooks.db\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/saket/Documents/GitHub/Pyhton/web%20scraping/test.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m cur \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mcursor()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/saket/Documents/GitHub/Pyhton/web%20scraping/test.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m cur\u001b[39m.\u001b[39;49mexecute(\u001b[39m\"\u001b[39;49m\u001b[39mSELECT * FROM audiobooks\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/saket/Documents/GitHub/Pyhton/web%20scraping/test.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m results \u001b[39m=\u001b[39m cur\u001b[39m.\u001b[39mfetchall()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/saket/Documents/GitHub/Pyhton/web%20scraping/test.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m results:\n",
      "\u001b[1;31mOperationalError\u001b[0m: no such table: audiobooks"
     ]
    }
   ],
   "source": [
    "# print the database\n",
    "conn = sqlite3.connect(\"audiobooks.db\")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT * FROM audiobooks\")\n",
    "results = cur.fetchall()\n",
    "for row in results:\n",
    "    print(row)\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = (\"https://m.media-amazon.com/images/I/517I-u-1NGL._SL500_.jpg\",\n",
    "        \"https://m.media-amazon.com/images/I/51POjQXrnVL._SL500_.jpg\",\n",
    "        \"https://m.media-amazon.com/images/I/51ywcR6OqkL._SL500_.jpg\",\n",
    "        \"https://m.media-amazon.com/images/I/51-D+0blRnS._SL500_.jpg\",\n",
    "        \"https://m.media-amazon.com/images/I/51Xt2BYA5vL._SL500_.jpg\",\n",
    "        \"https://m.media-amazon.com/images/I/61CyC23FFwL._SL500_.jpg\",\n",
    "        \"https://m.media-amazon.com/images/I/51g3AinAJNL._SL500_.jpg\",\n",
    "        \"https://m.media-amazon.com/images/I/51pCOQAUu4L._SL500_.jpg\",\n",
    "        \"https://m.media-amazon.com/images/I/51NyNt9PePS._SL500_.jpg\",\n",
    "        \"https://m.media-amazon.com/images/I/51qO2LV-ilL._SL500_.jpg\",\n",
    "        \"https://m.media-amazon.com/images/I/515rqFN7PJL._SL500_.jpg\",\n",
    "        \"https://m.media-amazon.com/images/I/61i0TYaZ9pL._SL500_.jpg\",\n",
    "        \"https://m.media-amazon.com/images/I/614otPUQ5bL._SL500_.jpg\",\n",
    "        \"https://m.media-amazon.com/images/I/51HAoKblnpL._SL500_.jpg\",\n",
    "        \"https://m.media-amazon.com/images/I/51TjnQD6ILL._SL500_.jpg\",\n",
    "        \"https://m.media-amazon.com/images/I/51FvcRRvyUL._SL500_.jpg\",\n",
    "        \"https://m.media-amazon.com/images/I/61tqfwb3YML._SL500_.jpg\",\n",
    "        \"https://m.media-amazon.com/images/I/61h58kY9lsL._SL500_.jpg\",\n",
    "        \"https://m.media-amazon.com/images/I/51B+tOzmoAL._SL500_.jpg\",\n",
    "        \"https://m.media-amazon.com/images/I/61ltix+WxRL._SL500_.jpg\",\n",
    "    )\n",
    "\n",
    "# download images\n",
    "for image_link in images:\n",
    "    image_name = image_link.split(\"/\")[-1]\n",
    "    with open(image_name, \"wb\") as f:\n",
    "        f.write(requests.get(image_link).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-10 00:46:51 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): www.audible.com:443\n",
      "2023-06-10 00:46:52 [urllib3.connectionpool] DEBUG: https://www.audible.com:443 \"GET /search?audible_programs=20956260011&author_author=&keywords=&narrator=full-cast&pageSize=100&publisher=&sort=review-rank&title=&ref=a_search_l1_audible_programs_0&pf_rd_p=daf0f1c8-2865-4989-87fb-15115ba5a6d2&pf_rd_r=3CSM3Q3AG46QRQ0TVK0F&pageLoadId=dELu6hUurPGV8fAu&creativeId=9648f6bf-4f29-4fb4-9489-33163c0bb63e HTTP/1.1\" 200 None\n",
      "2023-06-10 00:46:53 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 18968\n",
      "2023-06-10 00:46:53 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 18966\n",
      "2023-06-10 00:46:53 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 18968\n",
      "2023-06-10 00:46:53 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 18966\n",
      "2023-06-10 00:46:53 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 18966\n",
      "2023-06-10 00:46:53 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 18967\n",
      "2023-06-10 00:46:53 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 18966\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.01\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.01\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.004107905498062523\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.01\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgarian confidence = 0.0\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.10640842928139142\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.46931823669551065\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.01\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.01\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.004107905498062523\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.01\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgarian confidence = 0.0\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.10640842928139142\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.46931823669551065\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0\n",
      "2023-06-10 00:46:54 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://m.media-amazon.com/images/G/01/audibleweb/arya/navigation/audible_logo._V517446980_.svg\n",
      "https://m.media-amazon.com/images/I/517I-u-1NGL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/G/01/audibleweb/brickcity/1.0/player/spinner/spinner-black._V533714622_.svg\n",
      "https://m.media-amazon.com/images/I/51POjQXrnVL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/G/01/audibleweb/brickcity/1.0/player/spinner/spinner-black._V533714622_.svg\n",
      "https://m.media-amazon.com/images/I/51ywcR6OqkL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/G/01/audibleweb/brickcity/1.0/player/spinner/spinner-black._V533714622_.svg\n",
      "https://m.media-amazon.com/images/I/51-D+0blRnS._SL500_.jpg\n",
      "https://m.media-amazon.com/images/G/01/audibleweb/brickcity/1.0/player/spinner/spinner-black._V533714622_.svg\n",
      "https://m.media-amazon.com/images/I/51Xt2BYA5vL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/G/01/audibleweb/brickcity/1.0/player/spinner/spinner-black._V533714622_.svg\n",
      "https://m.media-amazon.com/images/I/61CyC23FFwL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/G/01/audibleweb/brickcity/1.0/player/spinner/spinner-black._V533714622_.svg\n",
      "https://m.media-amazon.com/images/I/51g3AinAJNL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/G/01/audibleweb/brickcity/1.0/player/spinner/spinner-black._V533714622_.svg\n",
      "https://m.media-amazon.com/images/I/51pCOQAUu4L._SL500_.jpg\n",
      "https://m.media-amazon.com/images/G/01/audibleweb/brickcity/1.0/player/spinner/spinner-black._V533714622_.svg\n",
      "https://m.media-amazon.com/images/I/51NyNt9PePS._SL500_.jpg\n",
      "https://m.media-amazon.com/images/G/01/audibleweb/brickcity/1.0/player/spinner/spinner-black._V533714622_.svg\n",
      "https://m.media-amazon.com/images/I/51qO2LV-ilL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/G/01/audibleweb/brickcity/1.0/player/spinner/spinner-black._V533714622_.svg\n",
      "https://m.media-amazon.com/images/I/515rqFN7PJL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/G/01/audibleweb/brickcity/1.0/player/spinner/spinner-black._V533714622_.svg\n",
      "https://m.media-amazon.com/images/I/61i0TYaZ9pL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/G/01/audibleweb/brickcity/1.0/player/spinner/spinner-black._V533714622_.svg\n",
      "https://m.media-amazon.com/images/I/614otPUQ5bL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/G/01/audibleweb/brickcity/1.0/player/spinner/spinner-black._V533714622_.svg\n",
      "https://m.media-amazon.com/images/I/51HAoKblnpL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/G/01/audibleweb/brickcity/1.0/player/spinner/spinner-black._V533714622_.svg\n",
      "https://m.media-amazon.com/images/I/51TjnQD6ILL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/G/01/audibleweb/brickcity/1.0/player/spinner/spinner-black._V533714622_.svg\n",
      "https://m.media-amazon.com/images/I/51FvcRRvyUL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/G/01/audibleweb/brickcity/1.0/player/spinner/spinner-black._V533714622_.svg\n",
      "https://m.media-amazon.com/images/I/61tqfwb3YML._SL500_.jpg\n",
      "https://m.media-amazon.com/images/G/01/audibleweb/brickcity/1.0/player/spinner/spinner-black._V533714622_.svg\n",
      "https://m.media-amazon.com/images/I/61h58kY9lsL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/G/01/audibleweb/brickcity/1.0/player/spinner/spinner-black._V533714622_.svg\n",
      "https://m.media-amazon.com/images/I/51B+tOzmoAL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/G/01/audibleweb/brickcity/1.0/player/spinner/spinner-black._V533714622_.svg\n",
      "https://m.media-amazon.com/images/I/61ltix+WxRL._SL500_.jpg\n",
      "https://m.media-amazon.com/images/G/01/audibleweb/brickcity/1.0/player/spinner/spinner-black._V533714622_.svg\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image\n",
    "\n",
    "# Define the URL of the website\n",
    "# url = generate_link # Replace this with your desired URL\n",
    "\n",
    "# # Make a request to the website and get the HTML content\n",
    "# response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "# html = response.text\n",
    "\n",
    "# # Parse the HTML content using BeautifulSoup\n",
    "# soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "\n",
    "response = requests.get(generate_link())\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "\n",
    "# Find all the img tags in the HTML content\n",
    "\n",
    "    # Open the image URL using requests and Pillow\n",
    "    # image = Image.open(requests.get(src, stream=True).raw)\n",
    "    \n",
    "    # Save the image to a folder with a unique name\n",
    "    # image.save(f\"image{i}.jpg\") # You can change the folder and file name as you wish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-10 00:41:19 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2023-06-10 00:41:19 [scrapy.utils.log] INFO: Versions: lxml 4.8.0.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.2.0, Python 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1n  15 Mar 2022), cryptography 3.4.8, Platform Windows-10-10.0.22621-SP0\n",
      "2023-06-10 00:41:19 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n"
     ]
    },
    {
     "ename": "ReactorAlreadyInstalledError",
     "evalue": "reactor already installed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mReactorAlreadyInstalledError\u001b[0m              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\saket\\Documents\\GitHub\\Pyhton\\web scraping\\test.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 110>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/saket/Documents/GitHub/Pyhton/web%20scraping/test.ipynb#X14sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m process \u001b[39m=\u001b[39m CrawlerProcess()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/saket/Documents/GitHub/Pyhton/web%20scraping/test.ipynb#X14sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m \u001b[39m# Run the spider\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/saket/Documents/GitHub/Pyhton/web%20scraping/test.ipynb#X14sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m process\u001b[39m.\u001b[39;49mcrawl(GithubSpider)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/saket/Documents/GitHub/Pyhton/web%20scraping/test.ipynb#X14sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m process\u001b[39m.\u001b[39mstart()\n",
      "File \u001b[1;32mc:\\Users\\saket\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py:205\u001b[0m, in \u001b[0;36mCrawlerRunner.crawl\u001b[1;34m(self, crawler_or_spidercls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(crawler_or_spidercls, Spider):\n\u001b[0;32m    202\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    203\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mThe crawler_or_spidercls argument cannot be a spider object, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    204\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mit must be a spider class (or a Crawler object)\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 205\u001b[0m crawler \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_crawler(crawler_or_spidercls)\n\u001b[0;32m    206\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_crawl(crawler, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\saket\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py:238\u001b[0m, in \u001b[0;36mCrawlerRunner.create_crawler\u001b[1;34m(self, crawler_or_spidercls)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(crawler_or_spidercls, Crawler):\n\u001b[0;32m    237\u001b[0m     \u001b[39mreturn\u001b[39;00m crawler_or_spidercls\n\u001b[1;32m--> 238\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_crawler(crawler_or_spidercls)\n",
      "File \u001b[1;32mc:\\Users\\saket\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py:313\u001b[0m, in \u001b[0;36mCrawlerProcess._create_crawler\u001b[1;34m(self, spidercls)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(spidercls, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    312\u001b[0m     spidercls \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspider_loader\u001b[39m.\u001b[39mload(spidercls)\n\u001b[1;32m--> 313\u001b[0m \u001b[39mreturn\u001b[39;00m Crawler(spidercls, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msettings, init_reactor\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\saket\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py:82\u001b[0m, in \u001b[0;36mCrawler.__init__\u001b[1;34m(self, spidercls, settings, init_reactor)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m         \u001b[39mfrom\u001b[39;00m \u001b[39mtwisted\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternet\u001b[39;00m \u001b[39mimport\u001b[39;00m default\n\u001b[1;32m---> 82\u001b[0m         default\u001b[39m.\u001b[39;49minstall()\n\u001b[0;32m     83\u001b[0m     log_reactor_info()\n\u001b[0;32m     84\u001b[0m \u001b[39mif\u001b[39;00m reactor_class:\n",
      "File \u001b[1;32mc:\\Users\\saket\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py:194\u001b[0m, in \u001b[0;36minstall\u001b[1;34m()\u001b[0m\n\u001b[0;32m    191\u001b[0m reactor \u001b[39m=\u001b[39m SelectReactor()\n\u001b[0;32m    192\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtwisted\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternet\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmain\u001b[39;00m \u001b[39mimport\u001b[39;00m installReactor\n\u001b[1;32m--> 194\u001b[0m installReactor(reactor)\n",
      "File \u001b[1;32mc:\\Users\\saket\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py:32\u001b[0m, in \u001b[0;36minstallReactor\u001b[1;34m(reactor)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtwisted\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternet\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtwisted.internet.reactor\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m sys\u001b[39m.\u001b[39mmodules:\n\u001b[1;32m---> 32\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mReactorAlreadyInstalledError(\u001b[39m\"\u001b[39m\u001b[39mreactor already installed\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m twisted\u001b[39m.\u001b[39minternet\u001b[39m.\u001b[39mreactor \u001b[39m=\u001b[39m reactor\n\u001b[0;32m     34\u001b[0m sys\u001b[39m.\u001b[39mmodules[\u001b[39m\"\u001b[39m\u001b[39mtwisted.internet.reactor\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m reactor\n",
      "\u001b[1;31mReactorAlreadyInstalledError\u001b[0m: reactor already installed"
     ]
    }
   ],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Import the Twisted reactor\n",
    "from twisted.internet import reactor\n",
    "\n",
    "# Define a class for your spider\n",
    "class GithubSpider(scrapy.Spider):\n",
    "  # Give your spider a name\n",
    "  name = \"github_spider\"\n",
    "  # Define a list of URLs to start scraping from\n",
    "  start_urls = [generate_link()]\n",
    "\n",
    "  # Define a method to parse the response from each URL\n",
    "  def parse(self, response):\n",
    "    # Find all the elements that contain the product details\n",
    "    products = response.xpath(\"//div[@class='bc-col-responsive bc-col-6']\")\n",
    "\n",
    "    # Loop through each product element and extract the details\n",
    "    for product in products:\n",
    "      # Try to find the title element and handle the exception if not found\n",
    "      try:\n",
    "        title = product.xpath(\".//h3/text()\").get().strip()\n",
    "      except AttributeError:\n",
    "        title = \"N/A\"\n",
    "      # Try to find the subtitle element and handle the exception if not found\n",
    "      try:\n",
    "        subtitle = product.xpath(\".//span[@class='subtitle']/text()\").get().strip()\n",
    "      except AttributeError:\n",
    "        subtitle = \"N/A\"\n",
    "      # Try to find the author element and handle the exception if not found\n",
    "      try:\n",
    "        author = product.xpath(\".//li[@class='authorLabel']/text()\").get().strip()\n",
    "      except AttributeError:\n",
    "        author = \"N/A\"\n",
    "      # Try to find the narrator element and handle the exception if not found\n",
    "      try:\n",
    "        narrator = product.xpath(\".//li[@class='narratorLabel']/text()\").get().strip()\n",
    "      except AttributeError:\n",
    "        narrator = \"N/A\"\n",
    "      try:\n",
    "        series = product.xpath(\".//li[@class='seriesLabel']/text()\").get().strip()\n",
    "      except AttributeError:\n",
    "        series = \"N/A\"\n",
    "      try:\n",
    "        length = product.xpath(\".//li[@class='runtimeLabel']/text()\").get().strip()\n",
    "      except AttributeError:\n",
    "        length = \"N/A\"\n",
    "      try:\n",
    "        release_date = product.xpath(\".//li[@class='releaseDateLabel']/text()\").get().strip()\n",
    "      except AttributeError:\n",
    "        release_date = \"N/A\"\n",
    "      try:\n",
    "        language = product.xpath(\".//li[@class='languageLabel']/text()\").get().strip()\n",
    "      except AttributeError:\n",
    "        language = \"N/A\"\n",
    "\n",
    "      try:\n",
    "        ratings = product.xpath(\".//li[@class='ratingsLabel']/text()\").get().strip()\n",
    "      except AttributeError:\n",
    "        ratings = \"N/A\"\n",
    "\n",
    "      # Try to find the summary element and handle the exception if not found\n",
    "      try:\n",
    "        summary = product.xpath(\".//p/text()\").get().strip()\n",
    "      except AttributeError:\n",
    "        summary = \"N/A\"\n",
    "      \n",
    "      # Try to find the image element and handle the exception if not found\n",
    "      try:\n",
    "        # Get the src attribute of the img tag\n",
    "        image = product.xpath(\".//img/@src\").get()\n",
    "      except AttributeError:\n",
    "        image = \"N/A\"\n",
    "\n",
    "      # Try to find the link element and handle the exception if not found\n",
    "      try:\n",
    "        link = product.xpath(\".//a/@href\").get()\n",
    "      except AttributeError:\n",
    "        link = \"N/A\"\n",
    "\n",
    "      # Create a dictionary with the product details\n",
    "      details_dict = {\n",
    "        \"title\": title,\n",
    "        \"subtitle\": subtitle,\n",
    "        \"author\": author,\n",
    "        \"narrator\": narrator,\n",
    "        \"series\": series,\n",
    "        \"length\": length,\n",
    "        \"release_date\": release_date,\n",
    "        \"language\": language,\n",
    "        \"ratings\": ratings,\n",
    "        \"summary\": summary,\n",
    "        \"image\": image, # Add this line\n",
    "        \"link\": link # Add this line\n",
    "      }\n",
    "\n",
    "      # Yield or return the dictionary\n",
    "      yield details_dict\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the CrawlerProcess: process\n",
    "process = CrawlerProcess()\n",
    "\n",
    "# Run the spider\n",
    "process.crawl(GithubSpider)\n",
    "process.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL '<function generate_link at 0x0000020522810A60>': No schema supplied. Perhaps you meant http://<function generate_link at 0x0000020522810A60>?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\saket\\Documents\\GitHub\\Pyhton\\web scraping\\test.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/saket/Documents/GitHub/Pyhton/web%20scraping/test.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m url \u001b[39m=\u001b[39m generate_link \u001b[39m# Replace this with your desired URL\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/saket/Documents/GitHub/Pyhton/web%20scraping/test.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Make a request to the website and get the HTML content\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/saket/Documents/GitHub/Pyhton/web%20scraping/test.ipynb#W1sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(url, headers\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mUser-Agent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mMozilla/5.0\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/saket/Documents/GitHub/Pyhton/web%20scraping/test.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m html \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mtext\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/saket/Documents/GitHub/Pyhton/web%20scraping/test.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Parse the HTML content using BeautifulSoup\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saket\\anaconda3\\lib\\site-packages\\requests\\api.py:76\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[39m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     75\u001b[0m kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m'\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 76\u001b[0m \u001b[39mreturn\u001b[39;00m request(\u001b[39m'\u001b[39m\u001b[39mget\u001b[39m\u001b[39m'\u001b[39m, url, params\u001b[39m=\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\saket\\anaconda3\\lib\\site-packages\\requests\\api.py:61\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 61\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\saket\\anaconda3\\lib\\site-packages\\requests\\sessions.py:528\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[39m# Create the Request.\u001b[39;00m\n\u001b[0;32m    516\u001b[0m req \u001b[39m=\u001b[39m Request(\n\u001b[0;32m    517\u001b[0m     method\u001b[39m=\u001b[39mmethod\u001b[39m.\u001b[39mupper(),\n\u001b[0;32m    518\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    526\u001b[0m     hooks\u001b[39m=\u001b[39mhooks,\n\u001b[0;32m    527\u001b[0m )\n\u001b[1;32m--> 528\u001b[0m prep \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_request(req)\n\u001b[0;32m    530\u001b[0m proxies \u001b[39m=\u001b[39m proxies \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m    532\u001b[0m settings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmerge_environment_settings(\n\u001b[0;32m    533\u001b[0m     prep\u001b[39m.\u001b[39murl, proxies, stream, verify, cert\n\u001b[0;32m    534\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\saket\\anaconda3\\lib\\site-packages\\requests\\sessions.py:456\u001b[0m, in \u001b[0;36mSession.prepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    453\u001b[0m     auth \u001b[39m=\u001b[39m get_netrc_auth(request\u001b[39m.\u001b[39murl)\n\u001b[0;32m    455\u001b[0m p \u001b[39m=\u001b[39m PreparedRequest()\n\u001b[1;32m--> 456\u001b[0m p\u001b[39m.\u001b[39;49mprepare(\n\u001b[0;32m    457\u001b[0m     method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod\u001b[39m.\u001b[39;49mupper(),\n\u001b[0;32m    458\u001b[0m     url\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49murl,\n\u001b[0;32m    459\u001b[0m     files\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mfiles,\n\u001b[0;32m    460\u001b[0m     data\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mdata,\n\u001b[0;32m    461\u001b[0m     json\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mjson,\n\u001b[0;32m    462\u001b[0m     headers\u001b[39m=\u001b[39;49mmerge_setting(request\u001b[39m.\u001b[39;49mheaders, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mheaders, dict_class\u001b[39m=\u001b[39;49mCaseInsensitiveDict),\n\u001b[0;32m    463\u001b[0m     params\u001b[39m=\u001b[39;49mmerge_setting(request\u001b[39m.\u001b[39;49mparams, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams),\n\u001b[0;32m    464\u001b[0m     auth\u001b[39m=\u001b[39;49mmerge_setting(auth, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauth),\n\u001b[0;32m    465\u001b[0m     cookies\u001b[39m=\u001b[39;49mmerged_cookies,\n\u001b[0;32m    466\u001b[0m     hooks\u001b[39m=\u001b[39;49mmerge_hooks(request\u001b[39m.\u001b[39;49mhooks, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhooks),\n\u001b[0;32m    467\u001b[0m )\n\u001b[0;32m    468\u001b[0m \u001b[39mreturn\u001b[39;00m p\n",
      "File \u001b[1;32mc:\\Users\\saket\\anaconda3\\lib\\site-packages\\requests\\models.py:316\u001b[0m, in \u001b[0;36mPreparedRequest.prepare\u001b[1;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[39m\"\"\"Prepares the entire request with the given parameters.\"\"\"\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_method(method)\n\u001b[1;32m--> 316\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_url(url, params)\n\u001b[0;32m    317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_headers(headers)\n\u001b[0;32m    318\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_cookies(cookies)\n",
      "File \u001b[1;32mc:\\Users\\saket\\anaconda3\\lib\\site-packages\\requests\\models.py:390\u001b[0m, in \u001b[0;36mPreparedRequest.prepare_url\u001b[1;34m(self, url, params)\u001b[0m\n\u001b[0;32m    387\u001b[0m     error \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mInvalid URL \u001b[39m\u001b[39m{0!r}\u001b[39;00m\u001b[39m: No schema supplied. Perhaps you meant http://\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    388\u001b[0m     error \u001b[39m=\u001b[39m error\u001b[39m.\u001b[39mformat(to_native_string(url, \u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m--> 390\u001b[0m     \u001b[39mraise\u001b[39;00m MissingSchema(error)\n\u001b[0;32m    392\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m host:\n\u001b[0;32m    393\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidURL(\u001b[39m\"\u001b[39m\u001b[39mInvalid URL \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m: No host supplied\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m url)\n",
      "\u001b[1;31mMissingSchema\u001b[0m: Invalid URL '<function generate_link at 0x0000020522810A60>': No schema supplied. Perhaps you meant http://<function generate_link at 0x0000020522810A60>?"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the soup object to a file\n",
    "with open(\"soup.html\", \"w\") as file:\n",
    "    file.write(str(soup))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\saket\\Documents\\GitHub\\Pyhton\\web scraping\\test.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/saket/Documents/GitHub/Pyhton/web%20scraping/test.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data[\u001b[39m5\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('title', \"We're Alive: A Story of Survival, the Third Season\")\n",
      "('subtitle', 'N/A')\n",
      "('author', 'By: Kc Wayland')\n",
      "('narrator', 'Narrated by: full cast')\n",
      "('series', 'Series: We’re Alive: A Story of Survival, Book 3')\n",
      "('length', 'Length: 11 hrs and 31 mins')\n",
      "('release_date', 'Release date: 03-18-13')\n",
      "('language', 'Language: English')\n",
      "('ratings', '5 out of 5 stars 1,432 ratings')\n",
      "('summary', 'N/A')\n"
     ]
    }
   ],
   "source": [
    "for audiobook in data[5:]:\n",
    "    for key, value in audiobook.items():\n",
    "        # print(key, value)\n",
    "        pass\n",
    "\n",
    "data5 = {'title': \"We're Alive: A Story of Survival, the Third Season\",\n",
    " 'subtitle': 'N/A',\n",
    " 'author': 'By:\\n                                    Kc Wayland',\n",
    " 'narrator': 'Narrated by:\\n                                      full cast',\n",
    " 'series': 'Series:\\n                                      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n        \\n            \\n                \\n                \\n            \\n            \\n        \\n        \\n            \\n        \\n        We’re Alive: A Story of Survival, Book 3',\n",
    " 'length': 'Length: 11 hrs and 31 mins',\n",
    " 'release_date': 'Release date:\\n                                    03-18-13',\n",
    " 'language': 'Language:\\n                                      English',\n",
    " 'ratings': '5 out of 5 stars\\n1,432 ratings',\n",
    " 'summary': 'N/A'}\n",
    "\n",
    "# Loop through the values in the dictionary\n",
    "for key, value in data5.items():\n",
    "  # Remove leading and trailing whitespaces\n",
    "  value = value.strip()\n",
    "  # Replace multiple \\n with a single space\n",
    "  value = value.replace(\"\\n\", \" \")\n",
    "  # Update the dictionary with the formatted value\n",
    "  data5[key] = value\n",
    "\n",
    "# Import the re module\n",
    "import re\n",
    "\n",
    "# Loop through the values in the dictionary\n",
    "for key, value in data5.items():\n",
    "  # Remove leading and trailing whitespaces\n",
    "  value = value.strip()\n",
    "  # Replace multiple whitespaces with a single space using re.sub\n",
    "  value = re.sub(\"\\s+\", \" \", value)\n",
    "  # Update the dictionary with the formatted value\n",
    "  data5[key] = value\n",
    "\n",
    "# Print the formatted dictionary\n",
    "for i in data5.items():\n",
    "  print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# # read a html page with beautiful soup\n",
    "# file = open('test.html')\n",
    "# soup = BeautifulSoup(file, 'html.parser')\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (475666453.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [6]\u001b[1;36m\u001b[0m\n\u001b[1;33m    text from the page\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.audible.com/search?audible_programs=20956260011&author_author=&keywords=&narrator=full-cast&publisher=&sort=review-rank&title=&ref=a_search_l1_audible_programs_0&pf_rd_p=daf0f1c8-2865-4989-87fb-15115ba5a6d2&pf_rd_r=3CSM3Q3AG46QRQ0TVK0F&pageLoadId=dELu6hUurPGV8fAu&creativeId=9648f6bf-4f29-4fb4-9489-33163c0bb63e'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_link(page=1, audible_programs=\"20956260011\", author_author=\"\", keywords=\"\", narrator=\"full-cast\", publisher=\"\", sort=\"review-rank\", title=\"\"):\n",
    "  base_url = \"https://www.audible.com/search?\"\n",
    "  params = {\n",
    "    \"audible_programs\": audible_programs,\n",
    "    \"author_author\": author_author,\n",
    "    \"keywords\": keywords,\n",
    "    \"narrator\": narrator,\n",
    "    \"publisher\": publisher,\n",
    "    \"sort\": sort,\n",
    "    \"title\": title,\n",
    "    \"ref\": \"a_search_l1_audible_programs_0\",\n",
    "    \"pf_rd_p\": \"daf0f1c8-2865-4989-87fb-15115ba5a6d2\",\n",
    "    \"pf_rd_r\": \"3CSM3Q3AG46QRQ0TVK0F\",\n",
    "    \"pageLoadId\": \"dELu6hUurPGV8fAu\",\n",
    "    \"creativeId\": \"9648f6bf-4f29-4fb4-9489-33163c0bb63e\"\n",
    "  }\n",
    "  if page > 1:\n",
    "    params[\"page\"] = page\n",
    "  query = \"&\".join([f\"{key}={value}\" for key, value in params.items()])\n",
    "  return base_url + query\n",
    "\n",
    "generate_link()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
